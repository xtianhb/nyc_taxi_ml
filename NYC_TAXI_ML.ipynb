{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nr600/Documents/Projects/Information_Technology/nyc_taxi_ml/NYC_TAXI_ML.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nr600/Documents/Projects/Information_Technology/nyc_taxi_ml/NYC_TAXI_ML.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpq\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nr600/Documents/Projects/Information_Technology/nyc_taxi_ml/NYC_TAXI_ML.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m archivo_parquet \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nr600/Documents/Projects/Information_Technology/nyc_taxi_ml/NYC_TAXI_ML.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tabla_parquet \u001b[39m=\u001b[39m pq\u001b[39m.\u001b[39;49mread_table(archivo_parquet)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nr600/Documents/Projects/Information_Technology/nyc_taxi_ml/NYC_TAXI_ML.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m df \u001b[39m=\u001b[39m tabla_parquet\u001b[39m.\u001b[39mto_pandas()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nr600/Documents/Projects/Information_Technology/nyc_taxi_ml/NYC_TAXI_ML.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mdtypes)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/parquet/core.py:2956\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   2949\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2950\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keyword is no longer supported with the new \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2951\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdatasets-based implementation. Specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2952\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_legacy_dataset=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to temporarily recover the old \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2953\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbehaviour.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2954\u001b[0m     )\n\u001b[1;32m   2955\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2956\u001b[0m     dataset \u001b[39m=\u001b[39m _ParquetDatasetV2(\n\u001b[1;32m   2957\u001b[0m         source,\n\u001b[1;32m   2958\u001b[0m         schema\u001b[39m=\u001b[39;49mschema,\n\u001b[1;32m   2959\u001b[0m         filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   2960\u001b[0m         partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[1;32m   2961\u001b[0m         memory_map\u001b[39m=\u001b[39;49mmemory_map,\n\u001b[1;32m   2962\u001b[0m         read_dictionary\u001b[39m=\u001b[39;49mread_dictionary,\n\u001b[1;32m   2963\u001b[0m         buffer_size\u001b[39m=\u001b[39;49mbuffer_size,\n\u001b[1;32m   2964\u001b[0m         filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[1;32m   2965\u001b[0m         ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes,\n\u001b[1;32m   2966\u001b[0m         pre_buffer\u001b[39m=\u001b[39;49mpre_buffer,\n\u001b[1;32m   2967\u001b[0m         coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit,\n\u001b[1;32m   2968\u001b[0m         thrift_string_size_limit\u001b[39m=\u001b[39;49mthrift_string_size_limit,\n\u001b[1;32m   2969\u001b[0m         thrift_container_size_limit\u001b[39m=\u001b[39;49mthrift_container_size_limit,\n\u001b[1;32m   2970\u001b[0m     )\n\u001b[1;32m   2971\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m   2972\u001b[0m     \u001b[39m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m     \u001b[39m# module is not available\u001b[39;00m\n\u001b[1;32m   2974\u001b[0m     \u001b[39mif\u001b[39;00m filters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/parquet/core.py:2507\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2503\u001b[0m \u001b[39mif\u001b[39;00m partitioning \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhive\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2504\u001b[0m     partitioning \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mHivePartitioning\u001b[39m.\u001b[39mdiscover(\n\u001b[1;32m   2505\u001b[0m         infer_dictionary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 2507\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mdataset(path_or_paths, filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   2508\u001b[0m                            schema\u001b[39m=\u001b[39;49mschema, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mparquet_format,\n\u001b[1;32m   2509\u001b[0m                            partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[1;32m   2510\u001b[0m                            ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/dataset.py:782\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    771\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    772\u001b[0m     schema\u001b[39m=\u001b[39mschema,\n\u001b[1;32m    773\u001b[0m     filesystem\u001b[39m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    778\u001b[0m     selector_ignore_prefixes\u001b[39m=\u001b[39mignore_prefixes\n\u001b[1;32m    779\u001b[0m )\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m _is_path_like(source):\n\u001b[0;32m--> 782\u001b[0m     \u001b[39mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    783\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(source, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(_is_path_like(elem) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/dataset.py:465\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    463\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[1;32m    464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_single_source(source, filesystem)\n\u001b[1;32m    467\u001b[0m options \u001b[39m=\u001b[39m FileSystemFactoryOptions(\n\u001b[1;32m    468\u001b[0m     partitioning\u001b[39m=\u001b[39mpartitioning,\n\u001b[1;32m    469\u001b[0m     partition_base_dir\u001b[39m=\u001b[39mpartition_base_dir,\n\u001b[1;32m    470\u001b[0m     exclude_invalid_files\u001b[39m=\u001b[39mexclude_invalid_files,\n\u001b[1;32m    471\u001b[0m     selector_ignore_prefixes\u001b[39m=\u001b[39mselector_ignore_prefixes\n\u001b[1;32m    472\u001b[0m )\n\u001b[1;32m    473\u001b[0m factory \u001b[39m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[39mformat\u001b[39m, options)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/dataset.py:441\u001b[0m, in \u001b[0;36m_ensure_single_source\u001b[0;34m(path, filesystem)\u001b[0m\n\u001b[1;32m    439\u001b[0m     paths_or_selector \u001b[39m=\u001b[39m [path]\n\u001b[1;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m    443\u001b[0m \u001b[39mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "archivo_parquet = \"\"\n",
    "\n",
    "tabla_parquet = pq.read_table(archivo_parquet)\n",
    "\n",
    "df = tabla_parquet.to_pandas()\n",
    "\n",
    "print(df.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
